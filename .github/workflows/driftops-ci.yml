name: DriftOps CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

# Required for GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  build:
    name: Build reports & upload
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Configure MLflow (file-based)
        run: echo "MLFLOW_TRACKING_URI=file://$GITHUB_WORKSPACE/mlruns" >> $GITHUB_ENV

      - name: Ensure reports dir
        run: mkdir -p reports

      # --- Seed tiny predictions if missing (so perf section always shows) ---
      - name: Seed predictions if missing
        if: ${{ always() }}
        run: |
          if [ ! -f reports/predictions.csv ] && [ ! -f reports/predictions_current.csv ] && [ ! -f data/predictions.csv ]; then
            cat << 'CSV' > reports/predictions.csv
y_true,y_score
0,0.10
1,0.90
0,0.20
1,0.80
CSV
          fi

      # --- Phase V: Performance metrics (AUROC/AUPRC) ---
      - name: Performance metrics
        if: ${{ always() }}
        run: python src/eval/performance_metrics.py

      # --- Phase III: Data prep ---
      - name: Data prep
        run: |
          python src/data_prep.py --scaler standard --outliers zscore
          ls -lah data | sed -n '1,200p'

      # --- Phase IV: Drift ---
      - name: Drift detector
        run: python src/monitors/drift_detector.py

      # --- Phase V: SHAP ---
      - name: SHAP explainability
        run: |
          python src/explain/shap_summary.py \
            --data data/data_prepared_current.csv \
            --out reports/shap_top_features.png \
            --topk 15

      # --- Phase V: Fairness (optional) ---
      - name: Fairness audit
        run: python src/eval/fairness_audit.py
        continue-on-error: true

      # --- Phase V: Trustworthy Audit (optional) ---
      - name: Build trustworthy audit
        run: python src/eval/make_trustworthy_audit.py
        continue-on-error: true

      # --- Phase VI: Policy gate (hard fail if violated) ---
      - name: Enforce policy gate
        run: python src/ops/policy_gate.py

      # --- Always build dashboard & upload artifacts (even on fail) ---
      - name: Build dashboard
        if: ${{ always() }}
        run: python src/reports_dashboard.py

      - name: Upload artifacts (zip)
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: driftops-reports
          path: |
            reports/**
            mlruns/**
            policy.yaml
          if-no-files-found: warn
          retention-days: 30

      # --- Pages artifact (publishes ./reports) ---
      - name: Upload Pages artifact
        if: ${{ always() }}
        uses: actions/upload-pages-artifact@v3
        with:
          path: reports

  deploy:
    # publish even if gate fails, so dashboard is visible
    if: ${{ always() }}
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

      - name: Echo Pages URL
        run: echo "PAGES_URL=${{ steps.deployment.outputs.page_url }}"