# src/api/validate_cli.py
# Purpose: End-to-end validation orchestrator for Clinical DriftOps.
# Guarantees artifacts in reports/:
#   - performance_metrics.json
#   - performance_metrics.csv
#   - api_fairness_metrics.csv
#   - api_fairness_report.md
#   - fairness_summary.json
#   - policy_gate_result.json
#   - shap_top_features.json
#   - regulatory_monitor.json        <-- NEW
#   - run_metadata.json              <-- NEW
#   - live_validation.json           (status strictly PASS/FAIL)

from __future__ import annotations

import argparse
import csv
import json
import sys
from pathlib import Path
from typing import Any, Dict

# Local imports
try:
    from src.eval import performance_metrics
except Exception:
    performance_metrics = None
try:
    from src.ops import policy_gate as policy_gate_mod
except Exception:
    policy_gate_mod = None
try:
    from src.explain import shap_stub
except Exception:
    shap_stub = None
try:
    from src.ops import regulatory_monitor
except Exception:
    regulatory_monitor = None
try:
    from src.ops import run_metadata
except Exception:
    run_metadata = None

REPORTS_DIR = Path("reports")

def _write_text(path: Path, text: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(text, encoding="utf-8")

def _write_json(path: Path, payload: Dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(payload, indent=2), encoding="utf-8")

def _write_csv_kv(path: Path, mapping: Dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8", newline="\n") as f:
        w = csv.writer(f)
        w.writerow(["metric", "value"])
        for k, v in mapping.items():
            w.writerow([k, v if v is not None else ""])

def _safe_performance(preds_path: str) -> Dict[str, Any]:
    perf_json = REPORTS_DIR / "performance_metrics.json"
    perf_csv  = REPORTS_DIR / "performance_metrics.csv"

    metrics: Dict[str, Any] = {"n": 0, "accuracy@0.5": None, "auroc": None, "ks_stat": None}
    try:
        if performance_metrics is None or not hasattr(performance_metrics, "main"):
            raise RuntimeError("performance_metrics.main unavailable")
        computed = performance_metrics.main(preds_path, str(REPORTS_DIR))
        if isinstance(computed, dict):
            metrics.update(computed)
    except Exception as e:
        _write_json(REPORTS_DIR / "validator_error.json", {"stage": "metrics", "error": repr(e)})

    if not perf_json.exists():
        _write_json(perf_json, metrics)
    if not perf_csv.exists():
        _write_csv_kv(perf_csv, metrics)

    return json.loads(perf_json.read_text(encoding="utf-8"))

def _ensure_fairness_placeholders() -> Dict[str, Any]:
    fair_csv  = REPORTS_DIR / "api_fairness_metrics.csv"
    fair_md   = REPORTS_DIR / "api_fairness_report.md"
    fair_json = REPORTS_DIR / "fairness_summary.json"

    if not fair_csv.exists():
        with fair_csv.open("w", encoding="utf-8", newline="\n") as f:
            w = csv.writer(f)
            w.writerow(["slice", "metric", "value"])
            w.writerow(["overall", "demographic_parity_ratio", "1.0"])

    if not fair_md.exists():
        _write_text(
            fair_md,
            "# Fairness Report\n\n"
            "_Placeholder generated by validator._\n\n"
            "- Slices: overall\n- Metrics: demographic_parity_ratio\n"
        )

    if not fair_json.exists():
        _write_json(
            fair_json,
            {
                "slices": ["overall"],
                "metrics": {"overall": {"demographic_parity_ratio": 1.0}},
                "note": "Placeholder fairness summary generated by validator.",
            },
        )

    return json.loads(fair_json.read_text(encoding="utf-8"))

def _run_policy_gate(perf: Dict[str, Any]) -> Dict[str, Any]:
    gate_json = REPORTS_DIR / "policy_gate_result.json"

    try:
        if policy_gate_mod is None or not hasattr(policy_gate_mod, "main"):
            raise RuntimeError("policy_gate.main unavailable")
        result = policy_gate_mod.main(out_dir=str(REPORTS_DIR), performance=perf)
        if isinstance(result, dict):
            if not gate_json.exists():
                _write_json(gate_json, result)
            return result
    except Exception as e:
        _write_json(REPORTS_DIR / "validator_error.json", {"stage": "gate", "error": repr(e)})

    stub = {"status": "FAIL", "reasons": ["Gate unavailable or error; failing closed."], "policy": "default"}
    _write_json(gate_json, stub)
    return stub

def _ensure_shap_placeholder() -> str | None:
    try:
        if shap_stub is None or not hasattr(shap_stub, "main"):
            raise RuntimeError("shap_stub.main unavailable")
        return shap_stub.main(out_dir=str(REPORTS_DIR))
    except Exception as e:
        _write_json(REPORTS_DIR / "validator_error.json", {"stage": "shap", "error": repr(e)})
        fallback = REPORTS_DIR / "shap_top_features.json"
        if not fallback.exists():
            _write_json(fallback, {
                "features": [{"name": "placeholder_feature", "mean_abs_impact": 0.0}],
                "note": "Fallback SHAP placeholder due to error."
            })
        return str(fallback)

def _emit_regulatory_monitor() -> None:
    try:
        if regulatory_monitor is None or not hasattr(regulatory_monitor, "main"):
            raise RuntimeError("regulatory_monitor.main unavailable")
        regulatory_monitor.main(out_dir=str(REPORTS_DIR))
    except Exception as e:
        _write_json(REPORTS_DIR / "validator_error.json", {"stage": "regulatory_monitor", "error": repr(e)})

def _emit_run_metadata() -> None:
    try:
        if run_metadata is None or not hasattr(run_metadata, "main"):
            raise RuntimeError("run_metadata.main unavailable")
        run_metadata.main(out_dir=str(REPORTS_DIR))
    except Exception as e:
        _write_json(REPORTS_DIR / "validator_error.json", {"stage": "run_metadata", "error": repr(e)})

def run(args: argparse.Namespace) -> int:
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)

    exit_code = 1
    live_status = "FAIL"
    performance: Dict[str, Any] = {}
    fairness: Dict[str, Any] = {}
    gate: Dict[str, Any] = {}

    try:
        performance = _safe_performance(args.preds)
        fairness = _ensure_fairness_placeholders()
        _ensure_shap_placeholder()
        gate = _run_policy_gate(performance)

        # NEW: governance & links
        _emit_regulatory_monitor()
        _emit_run_metadata()

        gate_status = str(gate.get("status", "")).upper()
        if gate_status == "PASS":
            live_status = "PASS"
            exit_code = 0
        else:
            live_status = "FAIL"
            exit_code = 1
    except Exception as e:
        live_status = "FAIL"
        exit_code = 1
        _write_json(REPORTS_DIR / "validator_error.json", {"stage": "validator", "error": repr(e)})
    finally:
        _write_json(REPORTS_DIR / "live_validation.json", {
            "status": live_status,
            "performance": performance,
            "gate": gate,
        })

    return exit_code

def main() -> int:
    ap = argparse.ArgumentParser(description="Clinical DriftOps Validator CLI")
    ap.add_argument("--preds", required=True, help="Path to predictions CSV (e.g., reports/predictions.csv)")
    args = ap.parse_args()
    return run(args)

if __name__ == "__main__":
    sys.exit(main())


